<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> LLM &amp; ML Architectures ‚Äî Background Knowledge | Ahmad M. Nazar </title> <meta name="author" content="Ahmad M. Nazar"> <meta name="description" content="Foundational concepts and architectures in modern machine learning and generative AI"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://anazar99.github.io/resources/llm-ml-architectures/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Ahmad</span> M. Nazar </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item active"> <a class="nav-link" href="/resources/">resources <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item active"> <a class="nav-link" href="/_pages/dropdown/">submenus </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">LLM &amp; ML Architectures ‚Äî Background Knowledge</h1> <p class="post-description">Foundational concepts and architectures in modern machine learning and generative AI</p> </header> <article> <p>Understanding the architectural foundations behind Large Language Models (LLMs) and other modern deep learning systems is essential for working with generative AI. This page summarizes key models, techniques, and landmark papers that have shaped the field ‚Äî from attention mechanisms to transformer backbones and multi-modal fusion networks.</p> <hr> <h2 id="-core-concepts-to-understand">üß† Core Concepts to Understand</h2> <p>Before diving into complex generative systems, it‚Äôs important to be grounded in the following fundamental architectures and techniques:</p> <h3 id="feedforward-neural-networks-fnns"><strong>Feedforward Neural Networks (FNNs)</strong></h3> <p>The simplest form of neural network where data flows in one direction ‚Äî from input to output ‚Äî through fully connected layers. Each neuron applies a weighted sum followed by a non-linear activation (e.g., ReLU). FNNs are foundational but limited in handling spatial or sequential data.</p> <h3 id="convolutional-neural-networks-cnns"><strong>Convolutional Neural Networks (CNNs)</strong></h3> <p>Primarily used for image and spatial data. CNNs apply filters (kernels) that scan across input features to detect local patterns (e.g., edges, textures). They reduce dimensionality while preserving spatial hierarchies and are widely used in vision-based generative models (like GANs or image-conditioned transformers).</p> <h3 id="recurrent-neural-networks-rnns--long-short-term-memory-networks-lstms"><strong>Recurrent Neural Networks (RNNs) &amp; Long Short-Term Memory Networks (LSTMs)</strong></h3> <p>Designed for sequential data such as time series or language. RNNs maintain a hidden state that updates as input progresses through time. LSTMs (Long Short-Term Memory networks) enhance RNNs by incorporating gates that regulate memory, solving the vanishing gradient problem and enabling longer-term dependencies.</p> <h3 id="transformers"><strong>Transformers</strong></h3> <p>The current foundation of most modern generative AI systems, including LLMs. Transformers use <strong>self-attention</strong> to process all input tokens in parallel (not sequentially) while still modeling dependencies. Their encoder-decoder structure enables powerful sequence modeling, both for understanding (e.g., BERT) and generation (e.g., GPT).</p> <h3 id="self-attention-mechanisms"><strong>Self-Attention Mechanisms</strong></h3> <p>A technique that lets a model dynamically weight the importance of different parts of the input relative to each token. For example, in a sentence, a word like ‚Äúit‚Äù may attend to ‚Äúcar‚Äù to resolve reference. Self-attention allows models to capture contextual relationships at any distance.</p> <h3 id="positional-encoding"><strong>Positional Encoding</strong></h3> <p>Since transformers process sequences in parallel, they need a way to encode <strong>order</strong>. Positional encodings inject sequence position information into input embeddings (using sinusoidal functions or learned vectors) so that the model understands ‚Äúwhat comes first‚Äù or ‚Äúwhat follows what.‚Äù</p> <h3 id="fine-tuning-and-transfer-learning"><strong>Fine-tuning and Transfer Learning</strong></h3> <p>Instead of training from scratch, models pre-trained on large corpora are adapted to specific tasks by fine-tuning. This enables efficient learning with smaller datasets and improves generalization. Transfer learning is a key enabler of modern LLM applications in new domains.</p> <h3 id="prompt-engineering-and-instruction-tuning"><strong>Prompt Engineering and Instruction Tuning</strong></h3> <p>For LLMs, performance can be shaped by the way inputs (prompts) are structured. Prompt engineering involves crafting inputs that guide the model to desirable outputs. Instruction tuning further enhances this by fine-tuning models on prompt‚Äìresponse pairs that follow task instructions ‚Äî enabling more robust, zero-shot behavior.</p> <hr> <h2 id="-foundational--emerging-papers">üìö Foundational &amp; Emerging Papers</h2> <table> <thead> <tr> <th>Title</th> <th>Authors</th> <th>Summary</th> </tr> </thead> <tbody> <tr> <td><a href="https://arxiv.org/abs/1706.03762" rel="external nofollow noopener" target="_blank">Attention Is All You Need (2017)</a></td> <td>Vaswani et al.</td> <td>Introduced the Transformer architecture using self-attention, the foundation for all major LLMs.</td> </tr> <tr> <td><a href="https://arxiv.org/abs/1810.04805" rel="external nofollow noopener" target="_blank">BERT (2018)</a></td> <td>Devlin et al.</td> <td>Proposed masked language modeling and bidirectional context encoding.</td> </tr> <tr> <td><a href="https://arxiv.org/abs/2005.14165" rel="external nofollow noopener" target="_blank">GPT-3 (2020)</a></td> <td>Brown et al.</td> <td>Demonstrated that large transformer models can perform few-shot tasks with minimal tuning.</td> </tr> <tr> <td><a href="https://arxiv.org/abs/2107.14795" rel="external nofollow noopener" target="_blank">Perceiver IO (2021)</a></td> <td>Jaegle et al.</td> <td>General-purpose model for arbitrary modality inputs.</td> </tr> <tr> <td><a href="https://arxiv.org/abs/2304.02643" rel="external nofollow noopener" target="_blank">Segment Anything (2023)</a></td> <td>Kirillov et al.</td> <td>Vision foundation model for universal image segmentation.</td> </tr> <tr> <td><a href="https://arxiv.org/abs/2402.06196" rel="external nofollow noopener" target="_blank">Large Language Models: A Survey (2023)</a></td> <td>Minaee et al.</td> <td>A general survey on LLMs.</td> </tr> <tr> <td><a href="https://osf.io/preprints/osf/m6gcn" rel="external nofollow noopener" target="_blank">A Cognitive Review of LLMs (2023)</a></td> <td>Aky√ºrek et al.</td> <td>Reviews LLM capabilities through a cognitive science lens, covering reasoning, memory, and abstraction.</td> </tr> <tr> <td><a href="https://arxiv.org/abs/2311.11797" rel="external nofollow noopener" target="_blank">Igniting Language Intelligence: The Hitchhiker‚Äôs Guide From Chain-of-Thought Reasoning to Language Agents (2023)</a></td> <td>Zhang et al.</td> <td>Survey of LLMs with Chain-of-Thought Reasoning and Language Agents.</td> </tr> <tr> <td><a href="https://arxiv.org/abs/2311.05232" rel="external nofollow noopener" target="_blank">Generative Agents: Interactive Simulacra of Human Behavior (2023)</a></td> <td>Park et al.</td> <td>Demonstrates multi-agent simulation with memory and planning, showing emergent social behavior from LLMs.</td> </tr> <tr> <td><a href="https://arxiv.org/abs/2310.19736" rel="external nofollow noopener" target="_blank">Evaluating Large Language Models: A Comprehensive Survey</a></td> <td>Guo et al</td> <td>Survey on evaluation techniques of LLMs</td> </tr> <tr> <td><a href="https://arxiv.org/abs/2311.16673" rel="external nofollow noopener" target="_blank">Large Language Models Meet Computer Vision: A Brief Survey (2023)</a></td> <td>Hamadi</td> <td>Survey on Visual LLMs</td> </tr> <tr> <td><a href="https://arxiv.org/abs/2402.18587" rel="external nofollow noopener" target="_blank">At the Dawn of Generative AI Era: A Tutorial-cum-Survey on New Frontiers in 6G Wireless Intelligence (2024)</a></td> <td>Celik et al.</td> <td>Comprehensive survey on generative AI models and architectures.</td> </tr> <tr> <td><a href="https://arxiv.org/abs/2502.17425" rel="external nofollow noopener" target="_blank">Introducing Visual Perception Token into Multimodal Large Language Model (2025)</a></td> <td>Yu et al.</td> <td>Enhancing visual mutlimodal LLMs with a visual token</td> </tr> </tbody> </table> <hr> <h2 id="-architectures-at-a-glance">üîç Architectures at a Glance</h2> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/transformer-block-480.webp 480w,/assets/img/transformer-block-800.webp 800w,/assets/img/transformer-block-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/transformer-block.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Transformer Encoder-Decoder" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/attention-mechanism-480.webp 480w,/assets/img/attention-mechanism-800.webp 800w,/assets/img/attention-mechanism-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/attention-mechanism.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Self-Attention Diagram" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/llm-stack-480.webp 480w,/assets/img/llm-stack-800.webp 800w,/assets/img/llm-stack-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/llm-stack.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="LLM Stack &amp; Fine-Tuning Pipeline" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Left: Transformer block. Middle: Self-attention schematic. Right: High-level view of LLM training and usage. </div> <hr> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> ¬© Copyright 2025 Ahmad M. Nazar. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>