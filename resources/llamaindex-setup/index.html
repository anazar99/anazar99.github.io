<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Setting Up LlamaIndex with Hugging Face and Ollama | Ahmad M. Nazar </title> <meta name="author" content="Ahmad M. Nazar"> <meta name="description" content="A unified setup for local and cloud-based LLMs using LlamaIndex"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://anazar99.github.io/resources/llamaindex-setup/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Ahmad</span> M. Nazar </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item active"> <a class="nav-link" href="/resources/">resources <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item active"> <a class="nav-link" href="/_pages/dropdown.html">submenus </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Setting Up LlamaIndex with Hugging Face and Ollama</h1> <p class="post-description">A unified setup for local and cloud-based LLMs using LlamaIndex</p> </header> <article> <p><strong>LlamaIndex</strong> (formerly GPT Index) is a powerful framework that bridges external data sources (e.g., PDFs, databases, APIs) with LLMs. It supports local models (like <strong>Ollama</strong>) and cloud-hosted models (like those on <strong>Hugging Face</strong>) via unified interfaces.</p> <p>This guide walks through setting up LlamaIndex for both <strong>Hugging Face LLMs</strong> and <strong>local Ollama models</strong>, ending with a full code example.</p> <h2 id="-why-use-llamaindex">üß† Why Use LlamaIndex?</h2> <p>LlamaIndex is a powerful and modular framework for building LLM-based systems that require structured reasoning, document retrieval, or agent orchestration. It stands out because of its <strong>granular control</strong>, <strong>clean abstractions</strong>, and <strong>seamless support for both local and hosted models</strong>.</p> <p>Here are key reasons to use LlamaIndex in your generative AI workflows:</p> <hr> <h3 id="-fine-grained-control-over-prompting--logic">‚úÖ Fine-Grained Control over Prompting &amp; Logic</h3> <p>LlamaIndex offers high <strong>granularity</strong> over how prompts are constructed, executed, and chained. You can:</p> <ul> <li>Customize <strong>prompt templates</strong> for specific submodules (e.g., summarization, synthesis, query generation)</li> <li>Adjust <strong>token limits</strong>, <strong>inference parameters</strong>, and <strong>intermediate steps</strong> without leaving the high-level framework</li> <li>Debug or trace reasoning pipelines at the <strong>sub-prompt level</strong>, which is harder with end-to-end LLM frameworks</li> </ul> <hr> <h3 id="-built-in-agent-management">ü§ñ Built-In Agent Management</h3> <p>LlamaIndex has a straightforward yet extensible <strong>agent management system</strong>:</p> <ul> <li>Define task-specific agents (e.g., planner, retriever, responder)</li> <li>Integrate tools, memory, and logic routing in a few lines</li> <li>Ideal for building modular, multi-agent environments without writing scaffolding code from scratch</li> </ul> <p>It‚Äôs an excellent fit for <strong>research workflows</strong>, <strong>academic prototypes</strong>, and <strong>production-grade agents</strong> alike.</p> <hr> <h3 id="-powerful-rag-retrieval-augmented-generation">üîç Powerful RAG (Retrieval-Augmented Generation)</h3> <p>LlamaIndex was originally designed for <strong>document-indexed LLMs</strong> and excels in:</p> <ul> <li>Ingesting and indexing structured/unstructured data (e.g., PDFs, databases, APIs)</li> <li>Building retrieval pipelines using vector stores like FAISS, Qdrant, Weaviate</li> <li>Injecting context into LLMs at runtime via clean abstractions (<code class="language-plaintext highlighter-rouge">QueryEngine</code>, <code class="language-plaintext highlighter-rouge">Retriever</code>, <code class="language-plaintext highlighter-rouge">Index</code>)</li> </ul> <p>Whether you‚Äôre querying large corpora or chaining retrieval with reasoning, LlamaIndex gives you full control.</p> <hr> <h3 id="-backend-agnostic-ollama-openai-hugging-face-etc">üîÅ Backend Agnostic (Ollama, OpenAI, Hugging Face, etc.)</h3> <p>You can swap backends without rewriting logic:</p> <ul> <li>Run <strong>Ollama</strong> locally for privacy and speed</li> <li>Use <strong>Hugging Face</strong> models for open-source access</li> <li>Integrate <strong>OpenAI</strong> or <strong>Anthropic</strong> for production-grade LLMs</li> </ul> <p>Everything works under a unified API.</p> <hr> <h3 id="-extensibility-and-modularity">üìê Extensibility and Modularity</h3> <p>LlamaIndex is easy to extend:</p> <ul> <li>Plug in your own retrievers, tools, tools, memory modules</li> <li>Compose advanced behaviors using <strong>runnable chains</strong>, <strong>callbacks</strong>, and <strong>multi-modal interfaces</strong> </li> </ul> <hr> <h3 id="-tldr-why-llamaindex">üöÄ TL;DR: Why LlamaIndex?</h3> <table> <thead> <tr> <th>Feature</th> <th>Benefit</th> </tr> </thead> <tbody> <tr> <td>Granular prompt control</td> <td>Customize how each part of the system reasons</td> </tr> <tr> <td>Easy agent setup</td> <td>Create modular, explainable agent workflows</td> </tr> <tr> <td>RAG pipelines</td> <td>State-of-the-art retrieval and context injection</td> </tr> <tr> <td>Backend flexibility</td> <td>Use local or hosted models with the same codebase</td> </tr> <tr> <td>Developer friendly</td> <td>Ideal for experimentation, debugging, and fast iteration</td> </tr> </tbody> </table> <hr> <h2 id="-step-1-install-llamaindex">üì¶ Step 1: Install LlamaIndex</h2> <p>Use pip to install the base library and Ollama + Hugging Face extensions:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip install llama-index llama-index-llms-ollama llama-index-llms-huggingface llama-index-embeddings-huggingface
</code></pre></div></div> <p>If you‚Äôre using vector stores or file loaders, you may also want:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip install llama-index-readers-file llama-index-vector-stores-faiss llama-index-vector-stores-chroma
</code></pre></div></div> <h2 id="-step-2a-use-ollama-backed-llms-locally">üêè Step 2a: Use Ollama-Backed LLMs Locally</h2> <p>Ensure that Ollama is installed and running (<a href="https://anazar99.github.io/resources/ollama-setup/">see the Ollama setup page</a>). Then use:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">llama_index.llms.ollama</span> <span class="kn">import</span> <span class="n">Ollama</span>

<span class="n">llm</span> <span class="o">=</span> <span class="nc">Ollama</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="sh">"</span><span class="s">llama3.2</span><span class="sh">"</span><span class="p">,</span> <span class="n">request_timeout</span><span class="o">=</span><span class="mf">120.0</span><span class="p">)</span>
</code></pre></div></div> <p>You can substitute ‚Äúllama3.2‚Äù with any model supported by Ollama, such as ‚Äúmistral‚Äù or ‚Äúgemma‚Äù.</p> <h2 id="-step-2b-set-up-hugging-face-api-key">üß† Step 2b: Set Up Hugging Face API Key</h2> <p>If using hosted models from Hugging Face (e.g., Falcon, Mistral, LLaMA 2), create a HuggingFace accoount and generate an access token to place in your environment (.env) file (optional) to be used to pull specific models directly from HuggingFace if you choose to follow that route. You can get your token from <a href="https://huggingface.co/settings/tokens" rel="external nofollow noopener" target="_blank">HuggingFace</a>.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">llama_index.llms.huggingface</span> <span class="kn">import</span> <span class="n">HuggingFaceLLM</span>

<span class="n">llm</span> <span class="o">=</span> <span class="nc">HuggingFaceLLM</span><span class="p">(</span>
    <span class="n">model_name</span><span class="o">=</span><span class="sh">"</span><span class="s">tiiuae/falcon-7b-instruct</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">context_window</span><span class="o">=</span><span class="mi">2048</span><span class="p">,</span>
    <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span>
    <span class="n">generate_kwargs</span><span class="o">=</span><span class="p">{</span><span class="sh">"</span><span class="s">temperature</span><span class="sh">"</span><span class="p">:</span> <span class="mf">0.7</span><span class="p">},</span>
<span class="p">)</span>
</code></pre></div></div> <h2 id="-step-4-creating-a-prompt-template-and-calling-the-llm">üß™ Step 4: Creating a Prompt Template and Calling the LLM</h2> <p>Here‚Äôs a full example of defining a custom prompt and invoking the LLM:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">llama_index.llms.ollama</span> <span class="kn">import</span> <span class="n">Ollama</span>
<span class="kn">from</span> <span class="n">llama_index.prompts</span> <span class="kn">import</span> <span class="n">PromptTemplate</span>

<span class="c1"># Instantiate the model (local)
</span><span class="n">llm</span> <span class="o">=</span> <span class="nc">Ollama</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="sh">"</span><span class="s">llama3</span><span class="sh">"</span><span class="p">,</span> <span class="n">request_timeout</span><span class="o">=</span><span class="mf">120.0</span><span class="p">)</span>

<span class="c1"># Define a custom prompt template
</span><span class="n">template</span> <span class="o">=</span> <span class="sh">"""</span><span class="s">
{Add your system template here that instructs the LLM on how to answer given the specific context and domain}

### Question: {input}
</span><span class="sh">"""</span>

<span class="n">prompt_template</span> <span class="o">=</span> <span class="nc">PromptTemplate</span><span class="p">(</span><span class="n">template</span><span class="p">)</span>

<span class="c1"># Use the LLM
</span><span class="n">response</span> <span class="o">=</span> <span class="n">llm</span><span class="p">.</span><span class="nf">complete</span><span class="p">(</span><span class="n">prompt_template</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="nb">input</span><span class="o">=</span><span class="sh">"</span><span class="s">What is retrieval-augmented generation?</span><span class="sh">"</span><span class="p">))</span>
<span class="nf">print</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>
</code></pre></div></div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> ¬© Copyright 2025 Ahmad M. Nazar. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>